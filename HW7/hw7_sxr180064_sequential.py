# -*- coding: utf-8 -*-
"""HW7_SXR180064_SEQUENTIAL.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1BFBFpq3jeGZwSfPSDgZ8GEtJ7vv8oFpV
"""

import tensorflow as tf
from tensorflow.keras.preprocessing.text import Tokenizer
from tensorflow.keras import layers, models

from sklearn.preprocessing import LabelEncoder
import numpy as np
import pandas as pd

np.random.seed(1234)

df = pd.read_csv('./ecommerceDataset.csv', header=None)
df.dropna(inplace=True)
df.drop_duplicates(inplace=True)
df

i = np.random.rand(len(df)) < 0.8
train = df[i]
test = df[~i]
print("train data size: ", train.shape)
print("test data size: ", test.shape)

num_labels = 2
vocab_size = 25000
batch_size = 100

train[1] = train[1].astype(str)

tokenizer = Tokenizer(num_words=vocab_size)
tokenizer.fit_on_texts(train[1])

x_train = tokenizer.texts_to_matrix(train[1], mode='tfidf')
x_test = tokenizer.texts_to_matrix(test[1], mode='tfidf')

encoder = LabelEncoder()
encoder.fit(train[0])
y_train = encoder.transform(train[0])
y_test = encoder.transform(test[0])

print("train shapes:", x_train.shape, y_train.shape)
print("test shapes:", x_test.shape, y_test.shape)

model = models.Sequential()
model.add(layers.Dense(32, input_dim=vocab_size, kernel_initializer='normal', activation='relu'))
model.add(layers.Dense(1, kernel_initializer='normal', activation='sigmoid'))
 
model.compile(loss='binary_crossentropy',
              optimizer='adam',
              metrics=['accuracy'])
 
history = model.fit(x_train, y_train,
                    batch_size=batch_size,
                    epochs=5,
                    verbose=1,
                    validation_split=0.2)

score = model.evaluate(x_test, y_test, batch_size=batch_size, verbose=1)
print('Accuracy: ', score[1])